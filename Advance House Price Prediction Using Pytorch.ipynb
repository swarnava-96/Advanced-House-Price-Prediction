{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ed3160c",
   "metadata": {},
   "source": [
    "# Advance House Price Prediction Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c8668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import torch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52c3497a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.5.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version check\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629fc629",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1940529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('houseprice.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
    "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8357f1b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>2003</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>Reg</td>\n",
       "      <td>1976</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2001</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>1915</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>IR1</td>\n",
       "      <td>2000</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass MSZoning  LotFrontage  LotArea Street LotShape  YearBuilt  \\\n",
       "0          60       RL         65.0     8450   Pave      Reg       2003   \n",
       "1          20       RL         80.0     9600   Pave      Reg       1976   \n",
       "2          60       RL         68.0    11250   Pave      IR1       2001   \n",
       "3          70       RL         60.0     9550   Pave      IR1       1915   \n",
       "4          60       RL         84.0    14260   Pave      IR1       2000   \n",
       "\n",
       "   1stFlrSF  2ndFlrSF  SalePrice  \n",
       "0       856       854     208500  \n",
       "1      1262         0     181500  \n",
       "2       920       866     223500  \n",
       "3       961       756     140000  \n",
       "4      1145      1053     250000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f5f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   object \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   object \n",
      " 5   LotShape     1201 non-null   object \n",
      " 6   YearBuilt    1201 non-null   int64  \n",
      " 7   1stFlrSF     1201 non-null   int64  \n",
      " 8   2ndFlrSF     1201 non-null   int64  \n",
      " 9   SalePrice    1201 non-null   int64  \n",
      "dtypes: float64(1), int64(6), object(3)\n",
      "memory usage: 103.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Lets check some informations about the features\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f554f4",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "457b3e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column name is MSSubClass and unique values are 15\n",
      "Column name is MSZoning and unique values are 5\n",
      "Column name is LotFrontage and unique values are 110\n",
      "Column name is LotArea and unique values are 869\n",
      "Column name is Street and unique values are 2\n",
      "Column name is LotShape and unique values are 4\n",
      "Column name is YearBuilt and unique values are 112\n",
      "Column name is 1stFlrSF and unique values are 678\n",
      "Column name is 2ndFlrSF and unique values are 368\n",
      "Column name is SalePrice and unique values are 597\n"
     ]
    }
   ],
   "source": [
    "# Lets see the unique values of all features\n",
    "for i in df.columns:\n",
    "    print(\"Column name is {} and unique values are {}\".format(i, len(df[i].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c1b63e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets handle the Year column\n",
    "import datetime\n",
    "datetime.datetime.now().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc0b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Total Years\"] = datetime.datetime.now().year - df[\"YearBuilt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cffc4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets drop the YearBuilt feature as it is not that important now\n",
    "df.drop(\"YearBuilt\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c1fdf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'LotShape', '1stFlrSF', '2ndFlrSF', 'SalePrice', 'Total Years'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the features now\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57409482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets seperate the categorical features and target feature\n",
    "cat_features = [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]\n",
    "out_feature = \"SalePrice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4e754dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lbl_encoders = {}\n",
    "for feature in cat_features:\n",
    "    lbl_encoders[feature] = LabelEncoder()\n",
    "    df[feature] = lbl_encoders[feature].fit_transform(df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3007959a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>SalePrice</th>\n",
       "      <th>Total Years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>208500</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>181500</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>223500</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>140000</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>250000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>175000</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>210000</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>266500</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>142125</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>147500</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1201 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  LotShape  1stFlrSF  \\\n",
       "0              5         3         65.0     8450       1         3       856   \n",
       "1              0         3         80.0     9600       1         3      1262   \n",
       "2              5         3         68.0    11250       1         0       920   \n",
       "3              6         3         60.0     9550       1         0       961   \n",
       "4              5         3         84.0    14260       1         0      1145   \n",
       "...          ...       ...          ...      ...     ...       ...       ...   \n",
       "1455           5         3         62.0     7917       1         3       953   \n",
       "1456           0         3         85.0    13175       1         3      2073   \n",
       "1457           6         3         66.0     9042       1         3      1188   \n",
       "1458           0         3         68.0     9717       1         3      1078   \n",
       "1459           0         3         75.0     9937       1         3      1256   \n",
       "\n",
       "      2ndFlrSF  SalePrice  Total Years  \n",
       "0          854     208500           18  \n",
       "1            0     181500           45  \n",
       "2          866     223500           20  \n",
       "3          756     140000          106  \n",
       "4         1053     250000           21  \n",
       "...        ...        ...          ...  \n",
       "1455       694     175000           22  \n",
       "1456         0     210000           43  \n",
       "1457      1152     266500           80  \n",
       "1458         0     142125           71  \n",
       "1459         0     147500           56  \n",
       "\n",
       "[1201 rows x 10 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the data\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4e996ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [5, 3, 1, 0],\n",
       "       ...,\n",
       "       [6, 3, 1, 3],\n",
       "       [0, 3, 1, 3],\n",
       "       [0, 3, 1, 3]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking and converting into tensors\n",
    "import numpy as np\n",
    "cat_features = np.stack([df[\"MSSubClass\"], df[\"MSZoning\"], df[\"Street\"], df[\"LotShape\"]], 1)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7182e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting numpy to tensors\n",
    "cat_features = torch.tensor(cat_features, dtype = torch.int64)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1171c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets seperate the continuous features\n",
    "cont_features = []\n",
    "for i in df.columns:\n",
    "    if i in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\", \"SalePrice\"]:\n",
    "        pass\n",
    "    else:\n",
    "        cont_features.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eb45b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'Total Years']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the continuous features\n",
    "cont_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15ff7a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   65.,  8450.,   856.,   854.,    18.],\n",
       "        [   80.,  9600.,  1262.,     0.,    45.],\n",
       "        [   68., 11250.,   920.,   866.,    20.],\n",
       "        ...,\n",
       "        [   66.,  9042.,  1188.,  1152.,    80.],\n",
       "        [   68.,  9717.,  1078.,     0.,    71.],\n",
       "        [   75.,  9937.,  1256.,     0.,    56.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking continuous features into tensor\n",
    "cont_values = np.stack([df[i].values for i in cont_features], axis = 1)\n",
    "cont_values = torch.tensor(cont_values, dtype = torch.float)\n",
    "cont_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84d48a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check the datatype\n",
    "cont_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ca3aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[208500.],\n",
       "        [181500.],\n",
       "        [223500.],\n",
       "        ...,\n",
       "        [266500.],\n",
       "        [142125.],\n",
       "        [147500.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets convert the target feature into tensor\n",
    "y = torch.tensor(df[\"SalePrice\"].values, dtype = torch.float).reshape(-1,1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a83cd457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1201 entries, 0 to 1459\n",
      "Data columns (total 10 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MSSubClass   1201 non-null   int64  \n",
      " 1   MSZoning     1201 non-null   int32  \n",
      " 2   LotFrontage  1201 non-null   float64\n",
      " 3   LotArea      1201 non-null   int64  \n",
      " 4   Street       1201 non-null   int32  \n",
      " 5   LotShape     1201 non-null   int32  \n",
      " 6   1stFlrSF     1201 non-null   int64  \n",
      " 7   2ndFlrSF     1201 non-null   int64  \n",
      " 8   SalePrice    1201 non-null   int64  \n",
      " 9   Total Years  1201 non-null   int64  \n",
      "dtypes: float64(1), int32(3), int64(6)\n",
      "memory usage: 89.1 KB\n"
     ]
    }
   ],
   "source": [
    "# Lets see the information of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2c22534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1201, 4]), torch.Size([1201, 5]), torch.Size([1201, 1]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the shapes of the categorical, continuous and the target features\n",
    "cat_features.shape, cont_values.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7301308",
   "metadata": {},
   "source": [
    "### Embedding size for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49bf210d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 5, 2, 4]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the unique values of each categorical feature\n",
    "cat_dims = [len(df[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]\n",
    "cat_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75f517b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15, 8), (5, 3), (2, 1), (4, 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets select the input and the output dimensions\n",
    "\n",
    "embedding_dim = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
    "embedding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d8eaee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Embedding(15, 8)\n",
       "  (1): Embedding(5, 3)\n",
       "  (2): Embedding(2, 1)\n",
       "  (3): Embedding(4, 2)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets import the torch libraries\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "embed_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc59b62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        ...,\n",
       "        [6, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [0, 3, 1, 3]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Categorical features\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7456a982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 3, 1, 3],\n",
       "        [0, 3, 1, 3],\n",
       "        [5, 3, 1, 0],\n",
       "        [6, 3, 1, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the first four rows\n",
    "catfeatures = cat_features[:4]\n",
    "catfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05cbd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "pd.set_option('display.max_rows', 500)\n",
    "embedding_val=[]\n",
    "for i,e in enumerate(embed_representation):\n",
    "    embedding_val.append(e(cat_features[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0476845e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.5918, -0.0265,  0.8832,  ..., -0.5375, -0.9647, -0.4923],\n",
       "         [-0.6486,  3.8315,  0.1318,  ...,  0.7655,  0.1837,  0.1132],\n",
       "         [-1.5918, -0.0265,  0.8832,  ..., -0.5375, -0.9647, -0.4923],\n",
       "         ...,\n",
       "         [ 0.7877, -0.8106,  0.5398,  ...,  1.5288,  0.9763, -0.2729],\n",
       "         [-0.6486,  3.8315,  0.1318,  ...,  0.7655,  0.1837,  0.1132],\n",
       "         [-0.6486,  3.8315,  0.1318,  ...,  0.7655,  0.1837,  0.1132]],\n",
       "        grad_fn=<EmbeddingBackward>),\n",
       " tensor([[-1.0007, -1.1900, -0.7499],\n",
       "         [-1.0007, -1.1900, -0.7499],\n",
       "         [-1.0007, -1.1900, -0.7499],\n",
       "         ...,\n",
       "         [-1.0007, -1.1900, -0.7499],\n",
       "         [-1.0007, -1.1900, -0.7499],\n",
       "         [-1.0007, -1.1900, -0.7499]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[0.2276],\n",
       "         [0.2276],\n",
       "         [0.2276],\n",
       "         ...,\n",
       "         [0.2276],\n",
       "         [0.2276],\n",
       "         [0.2276]], grad_fn=<EmbeddingBackward>),\n",
       " tensor([[ 0.0814,  0.1626],\n",
       "         [ 0.0814,  0.1626],\n",
       "         [-0.2939, -0.1433],\n",
       "         ...,\n",
       "         [ 0.0814,  0.1626],\n",
       "         [ 0.0814,  0.1626],\n",
       "         [ 0.0814,  0.1626]], grad_fn=<EmbeddingBackward>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the embedding_val\n",
    "embedding_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf7ffa3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5918, -0.0265,  0.8832,  ...,  0.2276,  0.0814,  0.1626],\n",
       "        [-0.6486,  3.8315,  0.1318,  ...,  0.2276,  0.0814,  0.1626],\n",
       "        [-1.5918, -0.0265,  0.8832,  ...,  0.2276, -0.2939, -0.1433],\n",
       "        ...,\n",
       "        [ 0.7877, -0.8106,  0.5398,  ...,  0.2276,  0.0814,  0.1626],\n",
       "        [-0.6486,  3.8315,  0.1318,  ...,  0.2276,  0.0814,  0.1626],\n",
       "        [-0.6486,  3.8315,  0.1318,  ...,  0.2276,  0.0814,  0.1626]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets stack them column wise\n",
    "z = torch.cat(embedding_val, axis = 1)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d72c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets implement dropout\n",
    "dropout = nn.Dropout(0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1274f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6529, -0.0441,  0.0000,  ...,  0.3793,  0.0000,  0.2711],\n",
       "        [-1.0810,  0.0000,  0.2196,  ...,  0.0000,  0.1357,  0.2711],\n",
       "        [-0.0000, -0.0000,  1.4721,  ...,  0.0000, -0.0000, -0.2389],\n",
       "        ...,\n",
       "        [ 1.3128, -1.3510,  0.0000,  ...,  0.3793,  0.0000,  0.0000],\n",
       "        [-0.0000,  6.3858,  0.0000,  ...,  0.0000,  0.0000,  0.2711],\n",
       "        [-1.0810,  6.3858,  0.0000,  ...,  0.3793,  0.1357,  0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets add the dropout to z\n",
    "final_embed = dropout(z)\n",
    "final_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29225820",
   "metadata": {},
   "source": [
    "### Creating a Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0f611bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p = 0.5 ):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
    "        self.emb_drop = nn.Dropout(p)\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
    "        \n",
    "        layerlist = []\n",
    "        n_emb = sum((out for inp,out in embedding_dim))\n",
    "        n_in = n_emb + n_cont\n",
    "        \n",
    "        for i in layers:\n",
    "            layerlist.append(nn.Linear(n_in,i))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            layerlist.append(nn.BatchNorm1d(i))\n",
    "            layerlist.append(nn.Dropout(p))\n",
    "            n_in = i\n",
    "            \n",
    "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layerlist)\n",
    "        \n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        embeddings = []\n",
    "        for i,e in enumerate(self.embeds):\n",
    "            embeddings.append(e(x_cat[:,i]))\n",
    "        x = torch.cat(embeddings, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        \n",
    "        x_cont = self.bn_cont(x_cont)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7486e2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(100)\n",
    "model = FeedForwardNN(embedding_dim,len(cont_features),1,[100,50],p=.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bb30b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cde34c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define loss and optimizer\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0af71118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "batch_size = 1200\n",
    "test_size = int(batch_size * 0.15)\n",
    "train_categorical = cat_features[: batch_size - test_size]\n",
    "test_categorical = cat_features[batch_size - test_size : batch_size]\n",
    "train_cont = cont_values[: batch_size - test_size]\n",
    "test_cont = cont_values[batch_size - test_size : batch_size]\n",
    "y_train = y[: batch_size - test_size]\n",
    "y_test = y[batch_size - test_size :batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36848226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 180, 1020, 180, 1020, 180)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets see the lengths of the above variables\n",
    "len(train_categorical), len(test_categorical), len(train_cont), len(test_cont), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d0dff86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1 and the Loss: 200496.78125\n",
      "Epoch number: 11 and the Loss: 200496.375\n",
      "Epoch number: 21 and the Loss: 200496.03125\n",
      "Epoch number: 31 and the Loss: 200495.75\n",
      "Epoch number: 41 and the Loss: 200495.515625\n",
      "Epoch number: 51 and the Loss: 200495.125\n",
      "Epoch number: 61 and the Loss: 200494.8125\n",
      "Epoch number: 71 and the Loss: 200494.515625\n",
      "Epoch number: 81 and the Loss: 200494.21875\n",
      "Epoch number: 91 and the Loss: 200493.875\n",
      "Epoch number: 101 and the Loss: 200493.484375\n",
      "Epoch number: 111 and the Loss: 200493.03125\n",
      "Epoch number: 121 and the Loss: 200492.78125\n",
      "Epoch number: 131 and the Loss: 200492.390625\n",
      "Epoch number: 141 and the Loss: 200491.953125\n",
      "Epoch number: 151 and the Loss: 200491.546875\n",
      "Epoch number: 161 and the Loss: 200491.046875\n",
      "Epoch number: 171 and the Loss: 200490.5625\n",
      "Epoch number: 181 and the Loss: 200490.109375\n",
      "Epoch number: 191 and the Loss: 200489.671875\n",
      "Epoch number: 201 and the Loss: 200489.046875\n",
      "Epoch number: 211 and the Loss: 200488.640625\n",
      "Epoch number: 221 and the Loss: 200487.9375\n",
      "Epoch number: 231 and the Loss: 200487.359375\n",
      "Epoch number: 241 and the Loss: 200486.703125\n",
      "Epoch number: 251 and the Loss: 200486.0625\n",
      "Epoch number: 261 and the Loss: 200485.359375\n",
      "Epoch number: 271 and the Loss: 200484.609375\n",
      "Epoch number: 281 and the Loss: 200484.109375\n",
      "Epoch number: 291 and the Loss: 200483.4375\n",
      "Epoch number: 301 and the Loss: 200482.46875\n",
      "Epoch number: 311 and the Loss: 200481.609375\n",
      "Epoch number: 321 and the Loss: 200481.140625\n",
      "Epoch number: 331 and the Loss: 200479.625\n",
      "Epoch number: 341 and the Loss: 200479.265625\n",
      "Epoch number: 351 and the Loss: 200478.484375\n",
      "Epoch number: 361 and the Loss: 200477.328125\n",
      "Epoch number: 371 and the Loss: 200476.40625\n",
      "Epoch number: 381 and the Loss: 200475.0\n",
      "Epoch number: 391 and the Loss: 200474.734375\n",
      "Epoch number: 401 and the Loss: 200473.40625\n",
      "Epoch number: 411 and the Loss: 200472.21875\n",
      "Epoch number: 421 and the Loss: 200471.328125\n",
      "Epoch number: 431 and the Loss: 200469.796875\n",
      "Epoch number: 441 and the Loss: 200468.890625\n",
      "Epoch number: 451 and the Loss: 200468.234375\n",
      "Epoch number: 461 and the Loss: 200466.21875\n",
      "Epoch number: 471 and the Loss: 200465.890625\n",
      "Epoch number: 481 and the Loss: 200464.25\n",
      "Epoch number: 491 and the Loss: 200462.5625\n",
      "Epoch number: 501 and the Loss: 200462.171875\n",
      "Epoch number: 511 and the Loss: 200460.21875\n",
      "Epoch number: 521 and the Loss: 200459.015625\n",
      "Epoch number: 531 and the Loss: 200457.609375\n",
      "Epoch number: 541 and the Loss: 200456.203125\n",
      "Epoch number: 551 and the Loss: 200454.546875\n",
      "Epoch number: 561 and the Loss: 200452.96875\n",
      "Epoch number: 571 and the Loss: 200451.234375\n",
      "Epoch number: 581 and the Loss: 200449.140625\n",
      "Epoch number: 591 and the Loss: 200447.796875\n",
      "Epoch number: 601 and the Loss: 200446.875\n",
      "Epoch number: 611 and the Loss: 200444.890625\n",
      "Epoch number: 621 and the Loss: 200443.375\n",
      "Epoch number: 631 and the Loss: 200440.78125\n",
      "Epoch number: 641 and the Loss: 200439.6875\n",
      "Epoch number: 651 and the Loss: 200437.6875\n",
      "Epoch number: 661 and the Loss: 200435.765625\n",
      "Epoch number: 671 and the Loss: 200434.640625\n",
      "Epoch number: 681 and the Loss: 200431.8125\n",
      "Epoch number: 691 and the Loss: 200429.25\n",
      "Epoch number: 701 and the Loss: 200427.53125\n",
      "Epoch number: 711 and the Loss: 200427.0625\n",
      "Epoch number: 721 and the Loss: 200424.8125\n",
      "Epoch number: 731 and the Loss: 200422.296875\n",
      "Epoch number: 741 and the Loss: 200420.234375\n",
      "Epoch number: 751 and the Loss: 200418.171875\n",
      "Epoch number: 761 and the Loss: 200416.421875\n",
      "Epoch number: 771 and the Loss: 200413.9375\n",
      "Epoch number: 781 and the Loss: 200411.65625\n",
      "Epoch number: 791 and the Loss: 200409.75\n",
      "Epoch number: 801 and the Loss: 200406.78125\n",
      "Epoch number: 811 and the Loss: 200404.96875\n",
      "Epoch number: 821 and the Loss: 200404.28125\n",
      "Epoch number: 831 and the Loss: 200401.25\n",
      "Epoch number: 841 and the Loss: 200397.578125\n",
      "Epoch number: 851 and the Loss: 200397.234375\n",
      "Epoch number: 861 and the Loss: 200393.125\n",
      "Epoch number: 871 and the Loss: 200390.96875\n",
      "Epoch number: 881 and the Loss: 200388.625\n",
      "Epoch number: 891 and the Loss: 200384.28125\n",
      "Epoch number: 901 and the Loss: 200384.921875\n",
      "Epoch number: 911 and the Loss: 200382.25\n",
      "Epoch number: 921 and the Loss: 200377.78125\n",
      "Epoch number: 931 and the Loss: 200374.671875\n",
      "Epoch number: 941 and the Loss: 200372.046875\n",
      "Epoch number: 951 and the Loss: 200369.96875\n",
      "Epoch number: 961 and the Loss: 200365.921875\n",
      "Epoch number: 971 and the Loss: 200363.84375\n",
      "Epoch number: 981 and the Loss: 200361.625\n",
      "Epoch number: 991 and the Loss: 200359.84375\n",
      "Epoch number: 1001 and the Loss: 200354.421875\n",
      "Epoch number: 1011 and the Loss: 200350.84375\n",
      "Epoch number: 1021 and the Loss: 200349.03125\n",
      "Epoch number: 1031 and the Loss: 200345.015625\n",
      "Epoch number: 1041 and the Loss: 200344.03125\n",
      "Epoch number: 1051 and the Loss: 200340.34375\n",
      "Epoch number: 1061 and the Loss: 200337.78125\n",
      "Epoch number: 1071 and the Loss: 200334.34375\n",
      "Epoch number: 1081 and the Loss: 200330.296875\n",
      "Epoch number: 1091 and the Loss: 200326.15625\n",
      "Epoch number: 1101 and the Loss: 200324.59375\n",
      "Epoch number: 1111 and the Loss: 200318.921875\n",
      "Epoch number: 1121 and the Loss: 200317.0\n",
      "Epoch number: 1131 and the Loss: 200317.265625\n",
      "Epoch number: 1141 and the Loss: 200309.859375\n",
      "Epoch number: 1151 and the Loss: 200306.03125\n",
      "Epoch number: 1161 and the Loss: 200304.421875\n",
      "Epoch number: 1171 and the Loss: 200299.265625\n",
      "Epoch number: 1181 and the Loss: 200296.828125\n",
      "Epoch number: 1191 and the Loss: 200291.984375\n",
      "Epoch number: 1201 and the Loss: 200291.328125\n",
      "Epoch number: 1211 and the Loss: 200285.921875\n",
      "Epoch number: 1221 and the Loss: 200284.4375\n",
      "Epoch number: 1231 and the Loss: 200275.5625\n",
      "Epoch number: 1241 and the Loss: 200272.8125\n",
      "Epoch number: 1251 and the Loss: 200265.234375\n",
      "Epoch number: 1261 and the Loss: 200264.625\n",
      "Epoch number: 1271 and the Loss: 200260.46875\n",
      "Epoch number: 1281 and the Loss: 200258.359375\n",
      "Epoch number: 1291 and the Loss: 200253.1875\n",
      "Epoch number: 1301 and the Loss: 200249.84375\n",
      "Epoch number: 1311 and the Loss: 200245.140625\n",
      "Epoch number: 1321 and the Loss: 200238.359375\n",
      "Epoch number: 1331 and the Loss: 200242.203125\n",
      "Epoch number: 1341 and the Loss: 200233.609375\n",
      "Epoch number: 1351 and the Loss: 200228.453125\n",
      "Epoch number: 1361 and the Loss: 200226.515625\n",
      "Epoch number: 1371 and the Loss: 200224.09375\n",
      "Epoch number: 1381 and the Loss: 200218.390625\n",
      "Epoch number: 1391 and the Loss: 200208.140625\n",
      "Epoch number: 1401 and the Loss: 200209.171875\n",
      "Epoch number: 1411 and the Loss: 200200.5625\n",
      "Epoch number: 1421 and the Loss: 200202.375\n",
      "Epoch number: 1431 and the Loss: 200199.15625\n",
      "Epoch number: 1441 and the Loss: 200188.28125\n",
      "Epoch number: 1451 and the Loss: 200188.3125\n",
      "Epoch number: 1461 and the Loss: 200180.96875\n",
      "Epoch number: 1471 and the Loss: 200176.578125\n",
      "Epoch number: 1481 and the Loss: 200175.84375\n",
      "Epoch number: 1491 and the Loss: 200164.5\n",
      "Epoch number: 1501 and the Loss: 200162.3125\n",
      "Epoch number: 1511 and the Loss: 200152.890625\n",
      "Epoch number: 1521 and the Loss: 200155.265625\n",
      "Epoch number: 1531 and the Loss: 200149.0625\n",
      "Epoch number: 1541 and the Loss: 200141.359375\n",
      "Epoch number: 1551 and the Loss: 200140.203125\n",
      "Epoch number: 1561 and the Loss: 200132.171875\n",
      "Epoch number: 1571 and the Loss: 200128.0625\n",
      "Epoch number: 1581 and the Loss: 200123.015625\n",
      "Epoch number: 1591 and the Loss: 200122.421875\n",
      "Epoch number: 1601 and the Loss: 200115.359375\n",
      "Epoch number: 1611 and the Loss: 200108.140625\n",
      "Epoch number: 1621 and the Loss: 200097.6875\n",
      "Epoch number: 1631 and the Loss: 200097.3125\n",
      "Epoch number: 1641 and the Loss: 200095.21875\n",
      "Epoch number: 1651 and the Loss: 200088.328125\n",
      "Epoch number: 1661 and the Loss: 200087.09375\n",
      "Epoch number: 1671 and the Loss: 200075.703125\n",
      "Epoch number: 1681 and the Loss: 200066.78125\n",
      "Epoch number: 1691 and the Loss: 200066.578125\n",
      "Epoch number: 1701 and the Loss: 200058.109375\n",
      "Epoch number: 1711 and the Loss: 200055.40625\n",
      "Epoch number: 1721 and the Loss: 200052.265625\n",
      "Epoch number: 1731 and the Loss: 200046.78125\n",
      "Epoch number: 1741 and the Loss: 200041.015625\n",
      "Epoch number: 1751 and the Loss: 200039.03125\n",
      "Epoch number: 1761 and the Loss: 200024.203125\n",
      "Epoch number: 1771 and the Loss: 200015.9375\n",
      "Epoch number: 1781 and the Loss: 200012.8125\n",
      "Epoch number: 1791 and the Loss: 200010.484375\n",
      "Epoch number: 1801 and the Loss: 200001.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch number: 1811 and the Loss: 199998.984375\n",
      "Epoch number: 1821 and the Loss: 199990.609375\n",
      "Epoch number: 1831 and the Loss: 199989.578125\n",
      "Epoch number: 1841 and the Loss: 199982.90625\n",
      "Epoch number: 1851 and the Loss: 199968.375\n",
      "Epoch number: 1861 and the Loss: 199971.453125\n",
      "Epoch number: 1871 and the Loss: 199961.796875\n",
      "Epoch number: 1881 and the Loss: 199952.453125\n",
      "Epoch number: 1891 and the Loss: 199942.125\n",
      "Epoch number: 1901 and the Loss: 199942.75\n",
      "Epoch number: 1911 and the Loss: 199944.3125\n",
      "Epoch number: 1921 and the Loss: 199932.234375\n",
      "Epoch number: 1931 and the Loss: 199924.90625\n",
      "Epoch number: 1941 and the Loss: 199915.140625\n",
      "Epoch number: 1951 and the Loss: 199913.6875\n",
      "Epoch number: 1961 and the Loss: 199907.265625\n",
      "Epoch number: 1971 and the Loss: 199899.484375\n",
      "Epoch number: 1981 and the Loss: 199901.078125\n",
      "Epoch number: 1991 and the Loss: 199885.0625\n",
      "Epoch number: 2001 and the Loss: 199881.703125\n",
      "Epoch number: 2011 and the Loss: 199868.4375\n",
      "Epoch number: 2021 and the Loss: 199865.328125\n",
      "Epoch number: 2031 and the Loss: 199859.15625\n",
      "Epoch number: 2041 and the Loss: 199852.609375\n",
      "Epoch number: 2051 and the Loss: 199839.84375\n",
      "Epoch number: 2061 and the Loss: 199839.5625\n",
      "Epoch number: 2071 and the Loss: 199838.359375\n",
      "Epoch number: 2081 and the Loss: 199823.109375\n",
      "Epoch number: 2091 and the Loss: 199821.34375\n",
      "Epoch number: 2101 and the Loss: 199815.34375\n",
      "Epoch number: 2111 and the Loss: 199805.59375\n",
      "Epoch number: 2121 and the Loss: 199790.859375\n",
      "Epoch number: 2131 and the Loss: 199795.890625\n",
      "Epoch number: 2141 and the Loss: 199785.59375\n",
      "Epoch number: 2151 and the Loss: 199778.015625\n",
      "Epoch number: 2161 and the Loss: 199776.71875\n",
      "Epoch number: 2171 and the Loss: 199773.515625\n",
      "Epoch number: 2181 and the Loss: 199766.609375\n",
      "Epoch number: 2191 and the Loss: 199752.15625\n",
      "Epoch number: 2201 and the Loss: 199741.65625\n",
      "Epoch number: 2211 and the Loss: 199735.0\n",
      "Epoch number: 2221 and the Loss: 199720.09375\n",
      "Epoch number: 2231 and the Loss: 199719.578125\n",
      "Epoch number: 2241 and the Loss: 199696.78125\n",
      "Epoch number: 2251 and the Loss: 199703.84375\n",
      "Epoch number: 2261 and the Loss: 199679.34375\n",
      "Epoch number: 2271 and the Loss: 199684.375\n",
      "Epoch number: 2281 and the Loss: 199681.046875\n",
      "Epoch number: 2291 and the Loss: 199670.546875\n",
      "Epoch number: 2301 and the Loss: 199660.734375\n",
      "Epoch number: 2311 and the Loss: 199659.078125\n",
      "Epoch number: 2321 and the Loss: 199652.890625\n",
      "Epoch number: 2331 and the Loss: 199658.015625\n",
      "Epoch number: 2341 and the Loss: 199639.0625\n",
      "Epoch number: 2351 and the Loss: 199619.1875\n",
      "Epoch number: 2361 and the Loss: 199616.25\n",
      "Epoch number: 2371 and the Loss: 199607.390625\n",
      "Epoch number: 2381 and the Loss: 199595.265625\n",
      "Epoch number: 2391 and the Loss: 199592.453125\n",
      "Epoch number: 2401 and the Loss: 199587.359375\n",
      "Epoch number: 2411 and the Loss: 199570.453125\n",
      "Epoch number: 2421 and the Loss: 199568.9375\n",
      "Epoch number: 2431 and the Loss: 199555.140625\n",
      "Epoch number: 2441 and the Loss: 199555.703125\n",
      "Epoch number: 2451 and the Loss: 199531.34375\n",
      "Epoch number: 2461 and the Loss: 199524.765625\n",
      "Epoch number: 2471 and the Loss: 199529.34375\n",
      "Epoch number: 2481 and the Loss: 199527.40625\n",
      "Epoch number: 2491 and the Loss: 199524.53125\n",
      "Epoch number: 2501 and the Loss: 199506.734375\n",
      "Epoch number: 2511 and the Loss: 199496.703125\n",
      "Epoch number: 2521 and the Loss: 199476.625\n",
      "Epoch number: 2531 and the Loss: 199491.84375\n",
      "Epoch number: 2541 and the Loss: 199473.921875\n",
      "Epoch number: 2551 and the Loss: 199468.171875\n",
      "Epoch number: 2561 and the Loss: 199456.5\n",
      "Epoch number: 2571 and the Loss: 199443.90625\n",
      "Epoch number: 2581 and the Loss: 199450.40625\n",
      "Epoch number: 2591 and the Loss: 199441.328125\n",
      "Epoch number: 2601 and the Loss: 199422.90625\n",
      "Epoch number: 2611 and the Loss: 199416.640625\n",
      "Epoch number: 2621 and the Loss: 199427.625\n",
      "Epoch number: 2631 and the Loss: 199386.171875\n",
      "Epoch number: 2641 and the Loss: 199385.09375\n",
      "Epoch number: 2651 and the Loss: 199376.328125\n",
      "Epoch number: 2661 and the Loss: 199371.6875\n",
      "Epoch number: 2671 and the Loss: 199354.296875\n",
      "Epoch number: 2681 and the Loss: 199345.71875\n",
      "Epoch number: 2691 and the Loss: 199328.921875\n",
      "Epoch number: 2701 and the Loss: 199341.0\n",
      "Epoch number: 2711 and the Loss: 199324.3125\n",
      "Epoch number: 2721 and the Loss: 199323.421875\n",
      "Epoch number: 2731 and the Loss: 199314.390625\n",
      "Epoch number: 2741 and the Loss: 199292.140625\n",
      "Epoch number: 2751 and the Loss: 199266.359375\n",
      "Epoch number: 2761 and the Loss: 199262.828125\n",
      "Epoch number: 2771 and the Loss: 199278.71875\n",
      "Epoch number: 2781 and the Loss: 199257.5625\n",
      "Epoch number: 2791 and the Loss: 199247.515625\n",
      "Epoch number: 2801 and the Loss: 199225.25\n",
      "Epoch number: 2811 and the Loss: 199234.484375\n",
      "Epoch number: 2821 and the Loss: 199204.8125\n",
      "Epoch number: 2831 and the Loss: 199212.03125\n",
      "Epoch number: 2841 and the Loss: 199194.25\n",
      "Epoch number: 2851 and the Loss: 199180.4375\n",
      "Epoch number: 2861 and the Loss: 199185.84375\n",
      "Epoch number: 2871 and the Loss: 199166.625\n",
      "Epoch number: 2881 and the Loss: 199173.921875\n",
      "Epoch number: 2891 and the Loss: 199155.53125\n",
      "Epoch number: 2901 and the Loss: 199133.15625\n",
      "Epoch number: 2911 and the Loss: 199116.25\n",
      "Epoch number: 2921 and the Loss: 199120.390625\n",
      "Epoch number: 2931 and the Loss: 199136.28125\n",
      "Epoch number: 2941 and the Loss: 199098.125\n",
      "Epoch number: 2951 and the Loss: 199101.40625\n",
      "Epoch number: 2961 and the Loss: 199079.28125\n",
      "Epoch number: 2971 and the Loss: 199056.5625\n",
      "Epoch number: 2981 and the Loss: 199057.921875\n",
      "Epoch number: 2991 and the Loss: 199050.59375\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "epochs = 3000\n",
    "final_losses = []\n",
    "for i in range(epochs):\n",
    "    i = i + 1\n",
    "    y_pred = model(train_categorical, train_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred, y_train))\n",
    "    final_losses.append(loss)\n",
    "    if i%10 == 1:\n",
    "        print(\"Epoch number: {} and the Loss: {}\".format(i,loss.item()))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cbe5ad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 189606.5\n"
     ]
    }
   ],
   "source": [
    "# Validating the test data\n",
    "y_pred = \"\"\n",
    "with torch.no_grad():\n",
    "    y_pred = model(test_categorical, test_cont)\n",
    "    loss = torch.sqrt(loss_function(y_pred,y_test))\n",
    "print(\"RMSE: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3c16440",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_verify=pd.DataFrame(y_test.tolist(),columns=[\"Test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a11f3c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>808.031250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1196.393066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>511.113098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2467.094727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>995.690613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1732.424561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1976.099121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2642.551270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>392.039642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4648.277832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1592.492432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1414.409058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1423.441162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-415.748291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2418.775146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-320.843292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1287.135986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>15.134706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2114.137451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-332.775604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1031.533813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>456.004456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>199.209732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1125.773926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>819.705261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>782.696045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>561.333435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3656.125732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1050.235474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1166.792358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-479.177094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>457.837616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2875.116455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-628.934692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1740.802246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>768.778320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>331.099548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>174.607758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1263.396484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-45.711407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2281.527588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2689.688721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-262.882141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1225.402100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1272.870483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-12.096446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>-154.283081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>522.356689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>24.857729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3359.235107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>364.281158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2910.823975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2054.309326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>742.366272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2656.990234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2141.335938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>206.519760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>366.940216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2064.691650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>963.100159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4196.768066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>-108.322159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1491.436646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2092.240723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1178.004883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>-5.648473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>628.849976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2067.771729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>-627.864929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2494.135986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>-1116.550049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>-843.385620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-598.970825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1994.677979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1330.195801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1614.141602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>381.680847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>-173.105667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>-1.820022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>-88.341797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>810.108948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>449.687866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>-821.098633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2148.978760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>-119.241653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>194.224503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>941.923035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>792.198914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1584.579346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-360.753815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>3235.798096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>302.987701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1341.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1031.755249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>-60.794731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>3980.019287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1721.163818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3623.356445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2949.447021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1985.479126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1041.440918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>-357.125153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2245.206787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>91.308205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1672.696533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>-415.863312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1301.019775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2586.349854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1629.203613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>2609.509277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>-837.491943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>970.387268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>-1059.863037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>988.306091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>-782.619507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>793.531311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>-205.312210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>-1008.779602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>3914.856689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1710.794434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2851.138916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>-180.614365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1492.296753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>86.913078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>-103.389885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>905.945190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1469.740356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4130.687012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>183.254715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>452.898376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>465.841125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>-615.931763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>1532.685669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>1200.242676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>2119.841309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>-235.208145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2291.572754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>-214.728836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>-188.745956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2645.324951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2298.499756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>647.467651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>-348.500946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>3258.119385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>627.450867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>1815.210938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>280.454895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>365.464325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>681.317200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>226.152634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>-27.266777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>826.363098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2208.999268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>550.152954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>-610.410645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>2922.229736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>-88.267929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>1400.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>807.239746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>822.394531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>-477.305023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>4390.449219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>-90.003349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>1207.527588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>2082.676514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>3175.255615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>1427.972046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>-0.546576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>3026.108643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>43.213936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>-1008.945557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>166.583923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>2133.593018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>236.466553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>1087.347168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1226.476440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1179.454590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2035.836548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>2334.789795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>620.317139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction\n",
       "0     808.031250\n",
       "1    1196.393066\n",
       "2     511.113098\n",
       "3    2467.094727\n",
       "4     995.690613\n",
       "5    1732.424561\n",
       "6    1976.099121\n",
       "7    2642.551270\n",
       "8     392.039642\n",
       "9    4648.277832\n",
       "10   1592.492432\n",
       "11   1414.409058\n",
       "12   1423.441162\n",
       "13   -415.748291\n",
       "14   2418.775146\n",
       "15   -320.843292\n",
       "16   1287.135986\n",
       "17     15.134706\n",
       "18   2114.137451\n",
       "19   -332.775604\n",
       "20   1031.533813\n",
       "21    456.004456\n",
       "22    199.209732\n",
       "23   1125.773926\n",
       "24    819.705261\n",
       "25    782.696045\n",
       "26    561.333435\n",
       "27   3656.125732\n",
       "28   1050.235474\n",
       "29   1166.792358\n",
       "30   -479.177094\n",
       "31    457.837616\n",
       "32   2875.116455\n",
       "33   -628.934692\n",
       "34   1740.802246\n",
       "35    768.778320\n",
       "36    331.099548\n",
       "37    174.607758\n",
       "38   1263.396484\n",
       "39    -45.711407\n",
       "40   2281.527588\n",
       "41   2689.688721\n",
       "42   -262.882141\n",
       "43   1225.402100\n",
       "44   1272.870483\n",
       "45    -12.096446\n",
       "46   -154.283081\n",
       "47    522.356689\n",
       "48     24.857729\n",
       "49   3359.235107\n",
       "50    364.281158\n",
       "51   2910.823975\n",
       "52   2054.309326\n",
       "53    742.366272\n",
       "54   2656.990234\n",
       "55   2141.335938\n",
       "56    206.519760\n",
       "57    366.940216\n",
       "58   2064.691650\n",
       "59    963.100159\n",
       "60   4196.768066\n",
       "61   -108.322159\n",
       "62   1491.436646\n",
       "63   2092.240723\n",
       "64   1178.004883\n",
       "65     -5.648473\n",
       "66    628.849976\n",
       "67   2067.771729\n",
       "68   -627.864929\n",
       "69   2494.135986\n",
       "70  -1116.550049\n",
       "71   -843.385620\n",
       "72   -598.970825\n",
       "73   1994.677979\n",
       "74   1330.195801\n",
       "75   1614.141602\n",
       "76    381.680847\n",
       "77   -173.105667\n",
       "78     -1.820022\n",
       "79    -88.341797\n",
       "80    810.108948\n",
       "81    449.687866\n",
       "82   -821.098633\n",
       "83   2148.978760\n",
       "84   -119.241653\n",
       "85    194.224503\n",
       "86    941.923035\n",
       "87    792.198914\n",
       "88   1584.579346\n",
       "89   -360.753815\n",
       "90   3235.798096\n",
       "91    302.987701\n",
       "92   1341.687500\n",
       "93   1031.755249\n",
       "94    -60.794731\n",
       "95   3980.019287\n",
       "96   1721.163818\n",
       "97   3623.356445\n",
       "98   2949.447021\n",
       "99   1985.479126\n",
       "100  1041.440918\n",
       "101  -357.125153\n",
       "102  2245.206787\n",
       "103    91.308205\n",
       "104  1672.696533\n",
       "105  -415.863312\n",
       "106  1301.019775\n",
       "107  2586.349854\n",
       "108  1629.203613\n",
       "109  2609.509277\n",
       "110  -837.491943\n",
       "111   970.387268\n",
       "112 -1059.863037\n",
       "113   988.306091\n",
       "114  -782.619507\n",
       "115   793.531311\n",
       "116  -205.312210\n",
       "117 -1008.779602\n",
       "118  3914.856689\n",
       "119  1710.794434\n",
       "120  2851.138916\n",
       "121  -180.614365\n",
       "122  1492.296753\n",
       "123    86.913078\n",
       "124  -103.389885\n",
       "125   905.945190\n",
       "126  1469.740356\n",
       "127  4130.687012\n",
       "128   183.254715\n",
       "129   452.898376\n",
       "130   465.841125\n",
       "131  -615.931763\n",
       "132  1532.685669\n",
       "133  1200.242676\n",
       "134  2119.841309\n",
       "135  -235.208145\n",
       "136  2291.572754\n",
       "137  -214.728836\n",
       "138  -188.745956\n",
       "139  2645.324951\n",
       "140  2298.499756\n",
       "141   647.467651\n",
       "142  -348.500946\n",
       "143  3258.119385\n",
       "144   627.450867\n",
       "145  1815.210938\n",
       "146   280.454895\n",
       "147   365.464325\n",
       "148   681.317200\n",
       "149   226.152634\n",
       "150   -27.266777\n",
       "151   826.363098\n",
       "152  2208.999268\n",
       "153   550.152954\n",
       "154  -610.410645\n",
       "155  2922.229736\n",
       "156   -88.267929\n",
       "157  1400.080200\n",
       "158   807.239746\n",
       "159   822.394531\n",
       "160  -477.305023\n",
       "161  4390.449219\n",
       "162   -90.003349\n",
       "163  1207.527588\n",
       "164  2082.676514\n",
       "165  3175.255615\n",
       "166  1427.972046\n",
       "167    -0.546576\n",
       "168  3026.108643\n",
       "169    43.213936\n",
       "170 -1008.945557\n",
       "171   166.583923\n",
       "172  2133.593018\n",
       "173   236.466553\n",
       "174  1087.347168\n",
       "175  1226.476440\n",
       "176  1179.454590\n",
       "177  2035.836548\n",
       "178  2334.789795\n",
       "179   620.317139"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_predicted=pd.DataFrame(y_pred.tolist(),columns=[\"Prediction\"])\n",
    "data_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b17cd1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130000.0</td>\n",
       "      <td>808.031250</td>\n",
       "      <td>129191.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138887.0</td>\n",
       "      <td>1196.393066</td>\n",
       "      <td>137690.606934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175500.0</td>\n",
       "      <td>511.113098</td>\n",
       "      <td>174988.886902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>195000.0</td>\n",
       "      <td>2467.094727</td>\n",
       "      <td>192532.905273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142500.0</td>\n",
       "      <td>995.690613</td>\n",
       "      <td>141504.309387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Test   Prediction     Difference\n",
       "0  130000.0   808.031250  129191.968750\n",
       "1  138887.0  1196.393066  137690.606934\n",
       "2  175500.0   511.113098  174988.886902\n",
       "3  195000.0  2467.094727  192532.905273\n",
       "4  142500.0   995.690613  141504.309387"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output\n",
    "final_output=pd.concat([data_verify,data_predicted],axis=1)\n",
    "final_output['Difference']=final_output['Test']-final_output['Prediction']\n",
    "final_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d0e45",
   "metadata": {},
   "source": [
    "### Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17cefde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SWARNAVA\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type FeedForwardNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,'HousePrice.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3de1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'HouseWeights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1c6db",
   "metadata": {},
   "source": [
    "### Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c19f88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_size=[(15, 8), (5, 3), (2, 1), (4, 2)]\n",
    "model1=FeedForwardNN(embs_size,5,1,[100,50],p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b2e36bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.load_state_dict(torch.load('HouseWeights.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36a93cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardNN(\n",
       "  (embeds): ModuleList(\n",
       "    (0): Embedding(15, 8)\n",
       "    (1): Embedding(5, 3)\n",
       "    (2): Embedding(2, 1)\n",
       "    (3): Embedding(4, 2)\n",
       "  )\n",
       "  (emb_drop): Dropout(p=0.4, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=19, out_features=100, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.4, inplace=False)\n",
       "    (4): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.4, inplace=False)\n",
       "    (8): Linear(in_features=50, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
